{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS146 Assignment 2: Solutions to optional exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stretch goal: Reparameterizing the normal likelihood\n",
    "1. The pdf of the normal distribution is\n",
    "   $$\n",
    "    p(x \\,|\\, \\mu,\\sigma^2)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{\\left(x-\\mu\\right)^2}{2\\sigma^2}\\right)\n",
    "   $$\n",
    "\n",
    "   Reparameterizing in terms of the precision involves replacing all instances of $\\frac{1}{\\sigma^2}$ with $\\tau$.\n",
    "   $$\n",
    "    p(x \\,|\\, \\mu,\\tau)=\\sqrt{\\frac{\\tau}{2\\pi}} \\exp\\left(-\\frac{\\tau}{2}\\left(x-\\mu\\right)^2\\right)\n",
    "   $$\n",
    "\n",
    "2. The Normal-Inverse-Gamma distribution has pdf\n",
    "   $$\n",
    "    p(x,\\sigma ^2 \\,|\\, \\mu,\\lambda,\\alpha,\\beta) = \\frac{\\sqrt{\\lambda} }{\\sigma \\sqrt{2\\pi }}\\frac{\\beta ^\\alpha }{\\Gamma (\\alpha) }\\left( \\frac{1}{\\sigma ^2}\\right )^{\\alpha +1} \\exp\\left(-\\frac{2\\beta +\\lambda (x-\\mu )^2}{2\\sigma ^2}\\right)\n",
    "   $$\n",
    "\n",
    "   Noting that this distribution is over random variables $x$ and $\\sigma^2$ we perform the substitution $\\tau=\\sigma^{-2}$.\n",
    "   \n",
    "   $$\\begin{align*}\n",
    "   p(x,\\sigma ^2 \\,|\\, \\mu,\\lambda,\\alpha,\\beta)\n",
    "   &= p(x,\\tau \\,|\\, \\mu,\\lambda,\\alpha,\\beta)\\left\\lvert\\frac{\\text{d}\\sigma^2}{\\text{d}\\tau}\\right\\rvert\\\\\n",
    "   &=\\frac{\\sqrt{\\tau}\\sqrt{\\lambda} }{ \\sqrt{2\\pi }}\\frac{\\beta ^\\alpha }{\\Gamma (\\alpha) }\\left( \\tau\\right )^{\\alpha +1} \\exp\\left(-\\frac{\\tau}{2}(2\\beta +\\lambda (x-\\mu )^2)\\right)\\left\\lvert-\\frac{1}{\\tau^2}\\right\\rvert\\\\\n",
    "   &=\\frac{\\beta ^\\alpha \\sqrt{\\lambda}}{\\Gamma (\\alpha) \\sqrt{2\\pi }} \\tau^{\\alpha -\\frac{1}{2}} \\exp\\left(-\\beta\\tau\\right) \\exp\\left(-\\frac{\\lambda\\tau (x-\\mu )^2}{2}\\right)\n",
    "   \\end{align*}$$\n",
    "   \n",
    "   Note that this is the pdf of the normal-gamma distribution.\n",
    "\n",
    "3. An informal argument for why we must multiply by the factor $\\left\\lvert\\frac{\\text{d}\\sigma^2}{\\text{d}\\tau}\\right\\rvert$ is that given 2 random variables $X, Y$ such that $Y =g(X)$ the cumulative distributions of $X$ and $Y$ have the relation\n",
    "    $$F_Y(y)=P(Y\\le y)=P(g(X) \\le y )=P(X \\le g^{-1}(y))=F_X (g^{-1}(y))$$\n",
    "    \n",
    "    Therefore, from the chain rule of differentiation, the probability densities of $X, Y$ have the relation\n",
    "    $$f_y(y)=F_Y^{'}(y)=\\frac{\\text{d}}{\\text{d}y}F_X (g^{-1}(y))=f_X (g^{-1}(y))\\frac{\\text{d}}{\\text{d}y}g^{-1}(y)$$\n",
    "    \n",
    "    Considering the cases where $g(X)$ is decreasing gives us the absolute value. Thus, when we are performing a transformation we need to multiply the transformed probability distributions with the derivative of the inverse transformative relation $\\frac{\\text{d}}{\\text{d}y}g^{-1}(y)$.\n",
    "\n",
    "    For a more intuitive explanation, when we transform the cumulative distribution, which tells us about real probabilities (the probability that the random variable is less than a certain value), we can simply use the transformation to know about this probability. However, the probability density does not tell us true probabilities. Informally, the density about a value of the random variable tells us the probability of the random variable in a small region about that value. Therefore, we need to account for how the transformation warps the random variable around that small region. Hence, we need to multiply the transformation by the derivative, which tells us the relationship between the new mapping and the old mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1a\n",
    "\n",
    "Generate 1000 samples from a normal distribution with mean 100 and standard\n",
    "deviation 10. How many of the numbers are at least 2 standard deviations away\n",
    "from the mean? How many to you expect to be at least 2 standard deviations away\n",
    "from the mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 numbers are at least 2 standard deviations away from the mean, empirically.\n",
      "45.5002638964 numbers are at least 2 standard deviations away from the mean, theoretically.\n"
     ]
    }
   ],
   "source": [
    "mean = 100\n",
    "sd = 10\n",
    "num_samples = 1000\n",
    "\n",
    "# Generate 1000 samples\n",
    "samples = np.random.normal(loc=mean, scale=sd, size=num_samples)\n",
    "\n",
    "# Calculate the absolute value of the difference from the samples to the mean\n",
    "two_sd_away = np.absolute(samples - mean) > 2 * sd\n",
    "print(\n",
    "    np.sum(two_sd_away),\n",
    "    \"numbers are at least 2 standard deviations away from the mean, empirically.\")\n",
    "\n",
    "# Two tailed cdf to find theoretical probability\n",
    "print(\n",
    "    ((1 - stats.norm.cdf(2)) * 2) * num_samples,\n",
    "    \"numbers are at least 2 standard deviations away from the mean, theoretically.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1b\n",
    "\n",
    "Toss a fair coin 50 times. How many heads do you have? How many heads to you\n",
    "expect to have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have 23 heads\n",
      "I expect to have 25.0 heads\n"
     ]
    }
   ],
   "source": [
    "tosses = 50\n",
    "p = 0.5  # Fair coin\n",
    "# Generate sample from the binomial distribution\n",
    "heads = np.random.binomial(tosses, p)\n",
    "print(\"I have\", heads, \"heads\")\n",
    "# Compute expected value\n",
    "print(\"I expect to have\", p * tosses, \"heads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1c \n",
    "\n",
    "Roll a 6-sided die 1000 times. How many 6s did you get? How many 6s do you\n",
    "expect to get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I rolled 171 sixes\n",
      "I expect to have 166.666666667 sixes\n"
     ]
    }
   ],
   "source": [
    "rolls = 1000\n",
    "sides = 6\n",
    "# Probability of getting each outcome\n",
    "p = np.ones(sides) / sides\n",
    "# 6s are the last element of the sample\n",
    "sixes = np.random.multinomial(rolls, p)[-1]\n",
    "print(\"I rolled\", sixes, \"sixes\")\n",
    "# Expected value of multinomial distribution\n",
    "print(\"I expect to have\", p[-1] * rolls, \"sixes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1d\n",
    "\n",
    "How much area (probability) is to the right of 1.5 for a normal distribution with mean\n",
    "0 and standard deviation 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22662735237686826"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The normal cumulative distribution ~ probability to the left\n",
    "1 - stats.norm.cdf(1.5, loc=0, scale=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Question 2\n",
    "\n",
    "Let $y$ be the number of 6s in 1000 rolls of a fair die.\n",
    "* Draw a sketch of the approximate distribution of $y$, based on the normal approximation.\n",
    "* Using the normal distribution function in SciPy, give approximate 5%, 25%, 50%, 75%, and 95% points for the distribution of $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y$ is a binomially distributed variable with a probability of success of $p = \\frac{1}{6}$. The normal approximation to the distribution of $y$ is a normal distribution with mean $$np = \\frac{1000}{6}$$ and standard deviation $$\\sqrt{np(1-p)} = \\sqrt{1000\\ \\frac{1}{6}\\ \\frac{5}{6}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEICAYAAAC0+DhzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhrDkYIwxZoj/H7nu0CPID2sWAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10bc53748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean = 1000 / 6\n",
    "sd = (1000 / 6 * 5 / 6) ** 0.5\n",
    "normal_approx = stats.norm(loc=mean, scale=sd)\n",
    "\n",
    "x = np.linspace(mean - 100, mean + 100, 1000)\n",
    "y = normal_approx.pdf(x)\n",
    "plt.plot(x, y)\n",
    "plt.title(\"Normal approximation to distribution of y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 5%, 25%, 50%, 75%, and 95% points for the distribution of y are respectively: 147.281880772 158.71772873 166.666666667 174.615604603 186.051452561\n"
     ]
    }
   ],
   "source": [
    "p5, p25, p50, p75, p95 = normal_approx.ppf([0.05, 0.25, 0.5, 0.75, 0.95])\n",
    "print(\n",
    "    \"The 5%, 25%, 50%, 75%, and 95% points for the distribution of y are respectively:\",\n",
    "    p5, p25, p50, p75, p95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "\n",
    "A random sample of $n$ students is drawn from a large population, and their weights are\n",
    "measured. The average weight of the sampled students is $\\hat{y} = 75 kg$. Assume the weights\n",
    "in the population are normally distributed with unknown mean $\\mu$ and known standard\n",
    "deviation 10 kg. Suppose your prior distribution for $\\mu$ is normal with mean 180 and\n",
    "standard deviation 40.\n",
    "1. Give your posterior distribution for $\\mu$. (Your answer will be a function of $n$.)\n",
    "    * Given a prior mean $\\mu_0$ and prior variance $\\sigma_0^2$, the posterior distribution is a normal distribution with posterior mean $$\\left(\\frac{1}{\\sigma_0^2}+\\frac{n}{\\sigma^2}\\right)^{-1} \\left(\\frac{\\mu_0}{\\sigma_0^2}+\\frac{\\sum_{i=1}^n x_i}{\\sigma^2}\\right)$$ and posterior variance $$\\left(\\frac{1}{\\sigma_0^2}+\\frac{n}{\\sigma^2}\\right)^{-1}.$$ These equations are in the [Wikipedia table of conjugate priors](https://en.wikipedia.org/wiki/Conjugate_prior#Table_of_conjugate_distributions) under \"normal\n",
    "with known variance\".\n",
    "    * In this example we have prior mean $\\mu_0 = 180$ and prior variance $\\sigma_0^2=1600$ and known variance $\\sigma^2 = 100$. The sum of the data points sufficient statistic $\\sum_{i=1}^n x_i$ can be obtained from the average weight of the sampled students $n\\hat{y} = 75n$\n",
    "    * Plugging this into our formula we have posterior mean $$\\left(\\frac{1}{1600}+\\frac{n}{100}\\right)^{-1} \\left(\\frac{180}{1600}+\\frac{75n}{100}\\right) = 75 + \\frac{105}{16n+1}.$$ Note that we can see the posterior mean is a compromise between the average obtained from data and our prior mean 180. As $n$ goes to infinity, we approach the mean obtained from the data. We have posterior variance $$\\left(\\frac{1}{1600}+\\frac{n}{100}\\right)^{-1} = \\frac{1600}{16n+1}.$$\n",
    "    * In summary, the posterior distribution for $\\mu$ is $$\\text{Normal}\\left(75 + \\frac{105}{16n+1}, \\frac{1600}{16n+1}\\right).$$\n",
    "\n",
    "2. A new student is sampled at random from the same population and has a weight of $y'$ pounds. Give a posterior predictive distribution for $y'$. (Your answer will still be a function of $n$.)\n",
    "    * The posterior predictive distribution is found by integrating out the parameter $\\mu$ in the joint parameter–new data distribution conditional on the old data.\n",
    "      $$p(y'\\,|\\,y)=\\int p(y'\\,|\\,\\theta)p(\\theta\\,|\\,y) d\\theta=\\int N(y'\\,|\\,\\mu,100) N(\\mu\\,|\\,\\mu_{n}, \\sigma_{n}^2) \\text{d}\\mu$$\n",
    "      Since the normal distribution is symmetric we can write\n",
    "      $$p(y'\\,|\\,y)=\\int N(\\mu\\,|\\,y',100) N(\\mu\\,|\\,\\mu_{n}, \\sigma_{n}^2) \\text{d}\\mu$$\n",
    "      where $\\mu_{n}, \\sigma_{n}^2$ are the posterior mean and variance.\n",
    "    * A product of two Gaussians is also a Gaussian. We have the following identity\n",
    "      $$N(x\\,|\\,\\mu_1, \\sigma_1^2)N(x\\,|\\,\\mu_2, \\sigma_2^2) = N\\left(\\mu_1\\,|\\,\\mu_2,\\sigma_1^2+\\sigma_2^2\\right) N\\left(\\mu\\,\\bigg\\vert\\,\\frac{\\mu_2\\sigma_1^2+\\mu_1\\sigma_2^2}{\\sigma_1^2+\\sigma_2^2},\\frac{1}{\\frac{1}{\\sigma_1^2}+\\frac{1}{\\sigma_2^2}}\\right)$$\n",
    "      For a proof of the identity, read this link http://www.tina-vision.net/docs/memos/2003-003.pdf\n",
    "    * We rewrite $$N(\\mu\\,|\\,y',100) N(\\mu\\,|\\,\\mu_{n}, \\sigma_{n}^2)=N\\left(y'\\,|\\,\\mu_n,100+\\sigma_n^2\\right) N\\left(\\mu\\,\\bigg\\vert\\,\\frac{\\mu_n 100+y'\\sigma_n^2}{100+\\sigma_n^2},\\frac{1}{\\frac{1}{100}+\\frac{1}{\\sigma_n^2}}\\right)$$\n",
    "    * Taking the integral of this expression, we have\n",
    "      $$\\begin{align*}\n",
    "      p(y'\\,|\\,y)\n",
    "      &= \\int N\\left(y'\\,|\\,\\mu_n,100+\\sigma_n^2\\right) N\\left(\\mu\\,\\bigg\\vert\\,\\frac{\\mu_n 100+y'\\sigma_n^2}{100+\\sigma_n^2},\\frac{1}{\\frac{1}{100}+\\frac{1}{\\sigma_n^2}}\\right) \\text{d}\\mu\\\\\n",
    "      &=N\\left(y'\\,|\\,\\mu_n,100+\\sigma_n^2\\right) \\int N\\left(\\mu\\,\\bigg\\vert\\,\\frac{\\mu_n 100+y'\\sigma_n^2}{100+\\sigma_n^2},\\frac{1}{\\frac{1}{100}+\\frac{1}{\\sigma_n^2}}\\right) \\text{d}\\mu\\\\\n",
    "      &=N\\left(y'\\,|\\,\\mu_n,100+\\sigma_n^2\\right)\n",
    "      \\end{align*}$$\n",
    "    * We can take the first normal pdf outside of the integral because it does not depend on $\\mu$. Furthermore, the integral over the second normal pdf vanishes because we take the integral over the support of the normal distribution — the pdf integrates to 1.\n",
    "\n",
    "3. For $n = 10$, give a 95% posterior interval for $\\theta$ and a 95% posterior predictive interval for $y'$.\n",
    "    * If $n=10$, the posterior mean is $75 + \\frac{105}{16\\times10+1} \\approx 75.65$ and the posterior variance is $\\frac{1600}{16\\times10+1} \\approx 9.94$.\n",
    "        * Thus the 95% posterior interval for theta is approximately $(75.65 - 1.96 \\sqrt {9.94}, 75.65 + 1.96 \\sqrt {9.94}) = (69.47, 81.83)$\n",
    "    * If $n=10$, the mean of the posterior predictive distribution is also 75.65, but the variance is $9.94+100=109.94$\n",
    "        * Thus the 95% posterior interval for y' is approximately $(75.65 - 1.96 \\sqrt {109.94}, 75.65 + 1.96 \\sqrt {109.94}) = (55.10, 96.20)$\n",
    "\n",
    "4. Do the same for $n = 100$.\n",
    "    * If $n=100$, the posterior mean is $75 + \\frac{105}{16\\times100+1} \\approx 75.07$ and the posterior variance is $\\frac{1600}{16\\times100+1} \\approx 0.999$.\n",
    "        * Thus the 95% posterior interval for theta is approximately $(75.07 - 1.96\\sqrt {0.999}, 75.07 + 1.96\\sqrt {0.999}) = (73.11, 77.03)$\n",
    "    * If $n=100$, the mean of the posterior predictive distribution is also 75.07, but the variance is $0.999+100=100.999$\n",
    "        * Thus the 95% posterior interval for y' is approximately $(75.07 - 1.96\\sqrt {100.999}, 75.65 + 1.96\\sqrt {100.999}) = (55.37, 94.77)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Question 4\n",
    "\n",
    "1. The likelihood of observing $y \\ge 100$ is\n",
    "    $$\\begin{align*}\n",
    "    P(y\\ge100\\,|\\,\\lambda)\n",
    "    &= \\int_{100}^{+\\infty} \\lambda e^{-\\lambda y }\\text{d}y\\\\\n",
    "    &= e^{-100\\lambda}\n",
    "    \\end{align*}$$\n",
    "\t* We have the posterior \n",
    "    $$\\begin{align*}\n",
    "    P(\\theta \\,|\\, y \\ge 100)\n",
    "    &\\propto P(y \\ge 100 \\,|\\, \\lambda) P(\\lambda)\\\\\n",
    "    &=e^{-100\\lambda} \\text{Gamma}(\\lambda\\,|\\,\\alpha,\\beta)\\\\\n",
    "    &\\propto e^{-100\\lambda }\\left [ \\lambda^{\\alpha -1}e^{-\\beta \\lambda} \\right ] \\\\\n",
    "    &= \\lambda^{\\alpha -1} e^{-100\\lambda -\\beta \\lambda} \\\\\n",
    "    &\\propto \\text{Gamma}(\\lambda\\,|\\,\\alpha,\\beta +100 )\n",
    "    \\end{align*}$$\n",
    "    * The posterior mean of $\\theta$ is\n",
    "    $$\\mathbb{E}(\\theta ) = \\frac{\\alpha }{\\beta + 100}$$\n",
    "    * The posterior variance of $\\theta$ is\n",
    "    $$\\mathbb{V}(\\theta ) = \\frac{\\alpha }{(\\beta + 100)^2}$$\n",
    "    \n",
    "2. Since the gamma prior is conjugate to the exponential likelihood, the posterior is also a gamma distribution.\n",
    "\n",
    "    $$\\text{Gamma}(\\lambda\\,|\\,\\alpha+1, \\beta +100)$$\n",
    "\n",
    "    * The posterior mean of $\\theta$ is\n",
    "    $$\\mathbb{E}(\\theta ) = \\frac{\\alpha + 1 }{\\beta + 100}$$\n",
    "    * The posterior variance of $\\theta$ is\n",
    "    $$\\mathbb{V}(\\theta ) = \\frac{\\alpha + 1}{(\\beta + 100)^2}$$\n",
    "    \n",
    "3. When we have more specific information we know only that the expected value of the posterior variance will decrease, not that the posterior variance will decrease in every case. \n",
    "\n",
    "   Qualitatively, the greater we observe $y$ to be, the more \"information\" we have received, because the probability of observing a high $y$ is so low. Thus, if we observe a large value of $y$, our Bayesian update leads us to believe that the true value of $\\lambda$ will likely be low and also to be more certain in this belief. Thus, even though we are given less information about the exact value of $y$ when we are told that $y \\ge 100$, we are given more information about the possible range of $y$. That is, we know that $y$ can be greater than 100. These are the two opposing forces that eventually lead to the posterior distribution of $\\lambda$ given our observation that $y \\ge 100$ being slightly less variable than our posterior distribution of $\\lambda$ given our observation that $y = 100$, even though *prima facie* we are given less information in the former case.\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
